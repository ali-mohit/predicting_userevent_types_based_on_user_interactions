# -*- coding: utf-8 -*-
"""Predicting_UserEventTypes_BasedOn_UserInteractions_With_Products.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Iy1rpKh0tbP23cBV7_9Ut90YV_voxIM

#Download DataSet from kaggle
`executation time= 10min`
"""

!pip install kaggle

!mkdir -p ~/.kaggle
!cp /content/kaggle.json ~/.kaggle/

# Download the dataset
!kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store

# Unzip the dataset
!unzip ecommerce-behavior-data-from-multi-category-store.zip -d /content/

# Verify the files have been copied to /content/sample_data
!ls /content/

!rm ecommerce-behavior-data-from-multi-category-store.zip

"""#Download Other datasets from other URLS
`executation time= 30min`

"""

import requests
import gzip
import shutil
import os
from concurrent.futures import ThreadPoolExecutor

urls = {
    "2019-Dec.csv.gz": "https://data.rees46.com/datasets/marketplace/2019-Dec.csv.gz",
    "2020-Jan.csv.gz": "https://data.rees46.com/datasets/marketplace/2020-Jan.csv.gz",
    "2020-Feb.csv.gz": "https://data.rees46.com/datasets/marketplace/2020-Feb.csv.gz",
    "2020-Mar.csv.gz": "https://data.rees46.com/datasets/marketplace/2020-Mar.csv.gz",
    "2020-Apr.csv.gz": "https://data.rees46.com/datasets/marketplace/2020-Apr.csv.gz"
}
# Directory to save the unzipped files
save_dir = "/content/"

# Function to download and unzip files
def download_and_unzip(file_name, url):
    output_path = save_dir + file_name.replace(".gz", "")
    gz_path = output_path + ".gz"

    # Download the file
    response = requests.get(url, stream=True)
    with open(gz_path, "wb") as gz_file:
        gz_file.write(response.content)

    # Unzip the file
    with gzip.open(gz_path, "rb") as f_in:
        with open(output_path, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)

    # Remove the .gz file after extraction
    os.remove(gz_path)

# Use ThreadPoolExecutor to download and unzip files concurrently
with ThreadPoolExecutor(max_workers=2) as executor:
    futures = [executor.submit(download_and_unzip, file_name, url) for file_name, url in urls.items()]
    for future in futures:
        future.result()  # Wait for all threads to complete

# Verify the files have been unzipped
!ls /content/

!head -n 1000000 2019-Oct.csv > 2019-Oct-sample.csv
!head -n 1000000 2019-Nov.csv > 2019-Nov-sample.csv
!head -n 1000000 2019-Dec.csv > 2019-Dec-sample.csv
!head -n 1000000 2020-Jan.csv > 2020-Jan-sample.csv
!head -n 1000000 2020-Feb.csv > 2020-Feb-sample.csv
!head -n 1000000 2020-Mar.csv > 2020-Mar-sample.csv
!head -n 1000000 2020-Apr.csv > 2020-Apr-sample.csv

"""#Reading All Datasets Using Dask

Because our datasets are more than 20GB, we are unable to use the pandas library, as pandas loads all records into memory. Therefore, we use Dask.

Dask is an open-source Python library for parallel computing. It scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem, including Pandas, scikit-learn, and NumPy.
"""

!pip install dask
!pip install dask-ml
!pip install imbalanced-learn
!pip install dask distributed
!pip install scikit-learn
!pip install tensorflow

import dask.dataframe as dd
import dask.array as da
from dask_ml.model_selection import train_test_split
from dask_ml.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import os
import joblib
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint

# Configure Dask to use available memory efficiently
import dask

dask.config.set({'distributed.worker.memory.target': 0.6,  # start spilling to disk at 60% memory usage
                 'distributed.worker.memory.spill': 0.7,   # spill to disk at 70% memory usage
                 'distributed.worker.memory.pause': 0.8,   # pause execution at 80% memory usage
                 'distributed.worker.memory.terminate': 0.95})  # terminate worker at 95% memory usage

# Create a Dask cluster with specific resources
cluster = LocalCluster(n_workers=2, threads_per_worker=2, memory_limit='3GB')  # Adjust these settings as needed
client = Client(cluster)

print(cluster)

import dask.dataframe as dd
import os

# Specify the directory containing the files
file_path = '/content/'

# List of CSV files
file_list = [
    #"2019-Oct.csv",
    #"2019-Nov.csv",
    #"2019-Dec.csv",
    #"2020-Jan.csv",
    #"2020-Feb.csv",
    #"2020-Mar.csv",
    #"2020-Apr.csv"
]

# List of CSV files
file_list = [
    "2019-Oct-sample.csv",
    "2019-Nov-sample.csv",
    "2019-Dec-sample.csv",
    "2020-Jan-sample.csv",
    "2020-Feb-sample.csv",
    "2020-Mar-sample.csv",
    "2020-Apr-sample.csv"
]

# Load and combine datasets using Dask
ddf = dd.read_csv([os.path.join(file_path, file) for file in file_list])

# Get basic statistics and missing values
data_info = ddf.info()
# missing_values = ddf.isnull().sum().compute()
# basic_statistics = ddf.describe().compute()

# Display the information
print(data_info)
#print("\nMissing Values:\n", missing_values)
#print("\nBasic Statistics:\n", basic_statistics)

"""# Cleaning data and Preprocessing Phase"""

ddf.head()

"""## Preprocess and adding custom feature into the data"""

# Handle missing values
ddf = ddf.fillna({'category_code': 'Unknown', 'brand': 'Unknown', 'user_session': 'Unknown'})

# Convert event_time to datetime and extract temporal features
ddf['event_time'] = dd.to_datetime(ddf['event_time'])
ddf['day_of_week'] = ddf['event_time'].dt.dayofweek
ddf['hour_of_day'] = ddf['event_time'].dt.hour

# Generate embeddings for brand feature using LabelEncoder
ddf['brand_encoded'] = dd.to_numeric(ddf['brand'], errors='coerce').fillna(-1).astype(int)

# Prepare the feature set
features = ddf[['price', 'day_of_week', 'hour_of_day', 'brand_encoded']]
target = ddf['event_type']

# Convert target to numerical labels
target = target.map({'view': 0, 'cart': 1, 'purchase': 2}).astype(int)

"""# Split the data into training and test sets"""

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=True)

# Standardize the features using Dask
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Compute the scaled arrays only once before applying SMOTE
X_train_np = X_train.compute()
y_train_np = y_train.compute()

"""# Apply SMOTE to handle class imbalance"""

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train_np, y_train_np)

"""# Incremental Training and Checkpointing"""

# Incremental Training and Checkpointing
n_chunks = 10
chunk_size = X_resampled.shape[0] // n_chunks

# Function to save model checkpoints
def save_checkpoint(model, filename):
    joblib.dump(model, filename)

"""#Train and checkpoint Logistic Regression model using SGDClassifier"""

# Train and checkpoint Logistic Regression model using SGDClassifier
logistic_model = SGDClassifier(max_iter=1000, tol=1e-3)
for i in range(n_chunks):
    start_idx = i * chunk_size
    end_idx = (i + 1) * chunk_size
    X_chunk = X_resampled[start_idx:end_idx]
    y_chunk = y_resampled[start_idx:end_idx]
    logistic_model.partial_fit(X_chunk, y_chunk, classes=np.unique(y_resampled))
    save_checkpoint(logistic_model, f'logistic_model_checkpoint_{i}.pkl')

"""# Train and checkpoint Neural Network model using MLPClassifier"""

# Train and checkpoint Neural Network model using MLPClassifier
nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)
for i in range(n_chunks):
    start_idx = i * chunk_size
    end_idx = (i + 1) * chunk_size
    X_chunk = X_resampled[start_idx:end_idx]
    y_chunk = y_resampled[start_idx:end_idx]
    nn_model.partial_fit(X_chunk, y_chunk, classes=np.unique(y_resampled))
    save_checkpoint(nn_model, f'nn_model_checkpoint_{i}.pkl')

"""# Train and checkpoint Deep Learning model using TensorFlow/Keras"""

y_resampled_categorical = to_categorical(y_resampled)
y_test_categorical = to_categorical(y_test.compute())

dl_model = Sequential([
    Dense(128, input_dim=X_resampled.shape[1], activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

dl_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""# Checkpoint callback"""

checkpoint_cb = ModelCheckpoint('dl_model_checkpoint.h5', save_best_only=True)
dl_model.fit(X_resampled, y_resampled_categorical, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint_cb])

"""# Evaluate the models"""

y_pred_logistic = logistic_model.predict(X_test.compute())
print("Logistic Regression Classifier Report:\n", classification_report(y_test.compute(), y_pred_logistic))
print("Logistic Regression Confusion Matrix:\n", confusion_matrix(y_test.compute(), y_pred_logistic))

y_pred_nn = nn_model.predict(X_test.compute())
print("Neural Network Classifier Report:\n", classification_report(y_test.compute(), y_pred_nn))
print("Neural Network Confusion Matrix:\n", confusion_matrix(y_test.compute(), y_pred_nn))

dl_model.load_weights('dl_model_checkpoint.h5')
y_pred_dl = dl_model.predict(X_test.compute())
y_pred_dl_classes = y_pred_dl.argmax(axis=1)
print("Deep Learning Classifier Report:\n", classification_report(y_test.compute(), y_pred_dl_classes))
print("Deep Learning Confusion Matrix:\n", confusion_matrix(y_test.compute(), y_pred_dl_classes))

"""# Train and checkpoint RandomForest model
(Note: RandomForest does not support incremental learning, so we train and save the full model)
"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_resampled, y_resampled)
save_checkpoint(rf_model, 'rf_model_checkpoint.pkl')

y_pred_rf = rf_model.predict(X_test.compute())
print("RandomForest Classifier Report:\n", classification_report(y_test.compute(), y_pred_rf))
print("RandomForest Confusion Matrix:\n", confusion_matrix(y_test.compute(), y_pred_rf))